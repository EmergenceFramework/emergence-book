{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Emergence is a way to build cloud software for organizations that aims to shift the focus from managing constellations of individual apps/services to managing a complete environment that truly belongs to and evolves with the organization. Emergence is not for building systems with ambitions of global scale. It is optimized for building, sharing, re-using, and extending systems in spaces where scaling means a global community of peer organizations getting to own their own systems and control their own futures while still benefiting from sharing an ecosystem of common starting points and additions.","title":"Introduction"},{"location":"#introduction","text":"Emergence is a way to build cloud software for organizations that aims to shift the focus from managing constellations of individual apps/services to managing a complete environment that truly belongs to and evolves with the organization. Emergence is not for building systems with ambitions of global scale. It is optimized for building, sharing, re-using, and extending systems in spaces where scaling means a global community of peer organizations getting to own their own systems and control their own futures while still benefiting from sharing an ecosystem of common starting points and additions.","title":"Introduction"},{"location":"SUMMARY/","text":"Table of contents \u00b6 Introduction Development Getting Started Using Studio Build a Console Command Build a Site-specific Service Deployment Deploy a Site with Chef Habitat Deploy a Site with Docker Guides Migrate a Site to a Holobranch Creating a Dynamic Page Make a Model Searchable by Tags Migrations Bootstrap a Site from Git Translating a Site into Multiple Languages Coming soon : Sending Emails","title":"Table of contents"},{"location":"SUMMARY/#table-of-contents","text":"Introduction Development Getting Started Using Studio Build a Console Command Build a Site-specific Service Deployment Deploy a Site with Chef Habitat Deploy a Site with Docker Guides Migrate a Site to a Holobranch Creating a Dynamic Page Make a Model Searchable by Tags Migrations Bootstrap a Site from Git Translating a Site into Multiple Languages Coming soon : Sending Emails","title":"Table of contents"},{"location":"deployment/docker/","text":"Deploy a Site with Docker \u00b6 There are two methods, both based on Chef Habitat plans for deploying an emergence site with Docker: Build Habitat packages for the app and composite and then have Habitat export a Docker container from the composite package Write a Dockerfile that handles building both Habitat packages internally and make deliberate use of Docker layers The former lets you stay out of the weeds of Docker container builds and provides versatile Habitat packages you can deploy in a variety of ways. The later lets you stay out of the weeds of Habitat and provides a hyper-optimized set of container image layers. Building from Habitat plans \u00b6 export HAB_ORIGIN = \"myorigin\" hab pkg build . hab pkg build --reuse habitat/composite env $( cat results/last_build.env | xargs ) bash -c 'hab pkg export docker results/$pkg_artifact' Building from a Dockerfile \u00b6 This Dockerfile can be used as-is for any emergence project repository with habitat/plan.sh and habitat/composite/plan.sh in place per the Deploy a Site with Chef Habitat guide. Replace every occurance of the placeholder myorigin with any origin name you\u2019d like to prefix your built packages with. In this scenario it need not be externally registered with any bldr server. There are also several occurances of the placeholder myapp that you should replace as well. # This Dockerfile is hyper-optimized to minimize layer changes FROM jarvus/habitat-compose:latest as habitat ARG HAB_LICENSE = no-accept ENV HAB_LICENSE = $HAB_LICENSE ENV STUDIO_TYPE = Dockerfile ENV HAB_ORIGIN = myorigin RUN hab origin key generate # pre-layer all external runtime plan deps COPY habitat/plan.sh /habitat/plan.sh RUN hab pkg install \\ core/bash \\ emergence/php-runtime \\ $( { cat '/habitat/plan.sh' && echo && echo 'echo \"${pkg_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # pre-layer all external runtime composite deps COPY habitat/composite/plan.sh /habitat/composite/plan.sh RUN hab pkg install \\ emergence/nginx \\ jarvus/habitat-compose \\ $( { cat '/habitat/composite/plan.sh' && echo && echo 'echo \"${pkg_deps[@]/$pkg_origin\\/*/} ${composite_mysql_pkg:-core/mysql}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / FROM habitat as projector # pre-layer all build-time plan deps RUN hab pkg install \\ core/hab-plan-build \\ jarvus/hologit \\ jarvus/toml-merge \\ $( { cat '/habitat/plan.sh' && echo && echo 'echo \"${pkg_build_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # pre-layer all build-time composite deps RUN hab pkg install \\ jarvus/toml-merge \\ $( { cat '/habitat/composite/plan.sh' && echo && echo 'echo \"${pkg_build_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # build application COPY . /src RUN hab pkg exec core/hab-plan-build hab-plan-build /src RUN hab pkg exec core/hab-plan-build hab-plan-build /src/habitat/composite FROM habitat as runtime # install .hart artifact from builder stage COPY --from = projector /hab/cache/artifacts/ $HAB_ORIGIN -* /hab/cache/artifacts/ RUN hab pkg install /hab/cache/artifacts/ $HAB_ORIGIN -* \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # configure persistent volumes RUN hab pkg exec core/coreutils mkdir -p '/hab/svc/mysql/data' '/hab/svc/myapp/data' \\ && hab pkg exec core/coreutils chown hab:hab -R '/hab/svc/mysql/data' '/hab/svc/myapp/data' VOLUME [ \"/hab/svc/mysql/data\" , \"/hab/svc/myapp/data\" ] # configure entrypoint ENTRYPOINT [ \"hab\" , \"sup\" , \"run\" ] CMD [ \"myorigin/myapp-composite\" ] With this file in place, you can run the following command to build a container without Habitat installed: docker build . \\ --build-arg HAB_LICENSE = accept-no-persist \\ -t myorigin/myapp The container could then be run like this: docker run \\ --name myapp \\ -d \\ -p 80 :80 \\ -e HAB_LICENSE = accept \\ myorigin/myapp Verify service status: docker exec myapp hab svc status","title":"Deploy a Site with Docker"},{"location":"deployment/docker/#deploy-a-site-with-docker","text":"There are two methods, both based on Chef Habitat plans for deploying an emergence site with Docker: Build Habitat packages for the app and composite and then have Habitat export a Docker container from the composite package Write a Dockerfile that handles building both Habitat packages internally and make deliberate use of Docker layers The former lets you stay out of the weeds of Docker container builds and provides versatile Habitat packages you can deploy in a variety of ways. The later lets you stay out of the weeds of Habitat and provides a hyper-optimized set of container image layers.","title":"Deploy a Site with Docker"},{"location":"deployment/docker/#building-from-habitat-plans","text":"export HAB_ORIGIN = \"myorigin\" hab pkg build . hab pkg build --reuse habitat/composite env $( cat results/last_build.env | xargs ) bash -c 'hab pkg export docker results/$pkg_artifact'","title":"Building from Habitat plans"},{"location":"deployment/docker/#building-from-a-dockerfile","text":"This Dockerfile can be used as-is for any emergence project repository with habitat/plan.sh and habitat/composite/plan.sh in place per the Deploy a Site with Chef Habitat guide. Replace every occurance of the placeholder myorigin with any origin name you\u2019d like to prefix your built packages with. In this scenario it need not be externally registered with any bldr server. There are also several occurances of the placeholder myapp that you should replace as well. # This Dockerfile is hyper-optimized to minimize layer changes FROM jarvus/habitat-compose:latest as habitat ARG HAB_LICENSE = no-accept ENV HAB_LICENSE = $HAB_LICENSE ENV STUDIO_TYPE = Dockerfile ENV HAB_ORIGIN = myorigin RUN hab origin key generate # pre-layer all external runtime plan deps COPY habitat/plan.sh /habitat/plan.sh RUN hab pkg install \\ core/bash \\ emergence/php-runtime \\ $( { cat '/habitat/plan.sh' && echo && echo 'echo \"${pkg_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # pre-layer all external runtime composite deps COPY habitat/composite/plan.sh /habitat/composite/plan.sh RUN hab pkg install \\ emergence/nginx \\ jarvus/habitat-compose \\ $( { cat '/habitat/composite/plan.sh' && echo && echo 'echo \"${pkg_deps[@]/$pkg_origin\\/*/} ${composite_mysql_pkg:-core/mysql}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / FROM habitat as projector # pre-layer all build-time plan deps RUN hab pkg install \\ core/hab-plan-build \\ jarvus/hologit \\ jarvus/toml-merge \\ $( { cat '/habitat/plan.sh' && echo && echo 'echo \"${pkg_build_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # pre-layer all build-time composite deps RUN hab pkg install \\ jarvus/toml-merge \\ $( { cat '/habitat/composite/plan.sh' && echo && echo 'echo \"${pkg_build_deps[@]/$pkg_origin\\/*/}\"' ; } | hab pkg exec core/bash bash ) \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # build application COPY . /src RUN hab pkg exec core/hab-plan-build hab-plan-build /src RUN hab pkg exec core/hab-plan-build hab-plan-build /src/habitat/composite FROM habitat as runtime # install .hart artifact from builder stage COPY --from = projector /hab/cache/artifacts/ $HAB_ORIGIN -* /hab/cache/artifacts/ RUN hab pkg install /hab/cache/artifacts/ $HAB_ORIGIN -* \\ && hab pkg exec core/coreutils rm -rf /hab/ { artifacts,src } / # configure persistent volumes RUN hab pkg exec core/coreutils mkdir -p '/hab/svc/mysql/data' '/hab/svc/myapp/data' \\ && hab pkg exec core/coreutils chown hab:hab -R '/hab/svc/mysql/data' '/hab/svc/myapp/data' VOLUME [ \"/hab/svc/mysql/data\" , \"/hab/svc/myapp/data\" ] # configure entrypoint ENTRYPOINT [ \"hab\" , \"sup\" , \"run\" ] CMD [ \"myorigin/myapp-composite\" ] With this file in place, you can run the following command to build a container without Habitat installed: docker build . \\ --build-arg HAB_LICENSE = accept-no-persist \\ -t myorigin/myapp The container could then be run like this: docker run \\ --name myapp \\ -d \\ -p 80 :80 \\ -e HAB_LICENSE = accept \\ myorigin/myapp Verify service status: docker exec myapp hab svc status","title":"Building from a Dockerfile"},{"location":"deployment/habitat/","text":"Deploy a Site with Chef Habitat \u00b6 Initialize a Chef Habitat plan for the PHP application: \u00b6 Create habitat/plan.sh pkg_name = myapp pkg_origin = myorigin pkg_maintainer = \"First Last <human@example.org>\" pkg_scaffolding = emergence/scaffolding-site pkg_version () { scaffolding_detect_pkg_version } Initialize default configuration for the PHP application: \u00b6 Create habitat/default.toml [sites.default] database = \"myapp\" Initialize a Chef Habitat plan for running PHP application + nginx + database: \u00b6 Create habitat/composite/plan.sh composite_app_pkg_name = myapp pkg_name = \" ${ composite_app_pkg_name } -composite\" pkg_origin = myorigin pkg_maintainer = \"First Last <human@example.org>\" pkg_scaffolding = emergence/scaffolding-composite # uncomment to use remote mysql instead of local core/mysql service: # composite_mysql_pkg=jarvus/mysql-remote pkg_version () { scaffolding_detect_pkg_version } Create habitat/composite/default.toml [services.app.config] default_timezone = \"America/New_York\" # declare a basic username+password for core/mysql [services.mysql.config] app_username = \"admin\" app_password = \"admin\" bind = \"0.0.0.0\" Open a Habitat Studio: \u00b6 HAB_DOCKER_OPTS = \"-p 7080:80\" hab studio enter -D","title":"Deploy a Site with Chef Habitat"},{"location":"deployment/habitat/#deploy-a-site-with-chef-habitat","text":"","title":"Deploy a Site with Chef Habitat"},{"location":"deployment/habitat/#initialize-a-chef-habitat-plan-for-the-php-application","text":"Create habitat/plan.sh pkg_name = myapp pkg_origin = myorigin pkg_maintainer = \"First Last <human@example.org>\" pkg_scaffolding = emergence/scaffolding-site pkg_version () { scaffolding_detect_pkg_version }","title":"Initialize a Chef Habitat plan for the PHP application:"},{"location":"deployment/habitat/#initialize-default-configuration-for-the-php-application","text":"Create habitat/default.toml [sites.default] database = \"myapp\"","title":"Initialize default configuration for the PHP application:"},{"location":"deployment/habitat/#initialize-a-chef-habitat-plan-for-running-php-application-nginx-database","text":"Create habitat/composite/plan.sh composite_app_pkg_name = myapp pkg_name = \" ${ composite_app_pkg_name } -composite\" pkg_origin = myorigin pkg_maintainer = \"First Last <human@example.org>\" pkg_scaffolding = emergence/scaffolding-composite # uncomment to use remote mysql instead of local core/mysql service: # composite_mysql_pkg=jarvus/mysql-remote pkg_version () { scaffolding_detect_pkg_version } Create habitat/composite/default.toml [services.app.config] default_timezone = \"America/New_York\" # declare a basic username+password for core/mysql [services.mysql.config] app_username = \"admin\" app_password = \"admin\" bind = \"0.0.0.0\"","title":"Initialize a Chef Habitat plan for running PHP application + nginx + database:"},{"location":"deployment/habitat/#open-a-habitat-studio","text":"HAB_DOCKER_OPTS = \"-p 7080:80\" hab studio enter -D","title":"Open a Habitat Studio:"},{"location":"deployment/restic/","text":"Set up Backups with Restic \u00b6 Install restic-snapshot command \u00b6 Install the emergence/restic-snapshot command: hab pkg install emergence/restic-snapshot Provision bucket \u00b6 Create a bucket and obtain access credentials at a low-cost cloud object storage host: Linode Add an Object Storage bucket Create an Access key Backblaze Provision B2 bucket Provision B2 app key Build restic environment \u00b6 Create a secure file to store needed environment variables for the restic client to read and write to the encrypted repository bucket: Linode #!/bin/bash RESTIC_REPOSITORY = \"s3:us-east-1.linodeobjects.com/myhost-restic\" RESTIC_PASSWORD = \"\" AWS_ACCESS_KEY_ID = \"\" # Access Key AWS_SECRET_ACCESS_KEY = \"\" # Secret Key Backblaze #!/bin/bash RESTIC_REPOSITORY = \"b2:myhost-restic\" RESTIC_PASSWORD = \"\" B2_ACCOUNT_ID = \"\" # keyID B2_ACCOUNT_KEY = \"\" # applicationKey Create /etc/restic.env from above template Tailor RESTIC_REPOSITORY to created bucket Generate RESTIC_PASSWORD and save to credential vault Fill storage credentials Secure configuration: sudo chmod 660 /etc/restic.env Initialize repository \u00b6 Load the environment into your current shell to run Restic\u2019s one-time init command to set up the encrypted repository structure within the bucket: set -a ; source /etc/restic.env ; set +a hab pkg exec emergence/restic-snapshot restic init Create backup script \u00b6 Create /etc/cron.daily/emergence-restic-backup #!/bin/bash /bin/hab pkg exec emergence/restic-snapshot snapshot Max executable: chmod +x /etc/cron.daily/emergence-restic-backup Run manually \u00b6 To verify the configuration and create an initial snapshot: /etc/cron.daily/emergence-restic-backup Verifying backups \u00b6 Load environment: set -a ; source /etc/restic.env ; set +a List snapshots: hab pkg exec emergence/restic-snapshot restic snapshots Examine contents of an SQL dump: hab pkg exec emergence/restic-snapshot restic dump ced0825f /database.sql | grep '^CREATE'","title":"Set up Backups with Restic"},{"location":"deployment/restic/#set-up-backups-with-restic","text":"","title":"Set up Backups with Restic"},{"location":"deployment/restic/#install-restic-snapshot-command","text":"Install the emergence/restic-snapshot command: hab pkg install emergence/restic-snapshot","title":"Install restic-snapshot command"},{"location":"deployment/restic/#provision-bucket","text":"Create a bucket and obtain access credentials at a low-cost cloud object storage host: Linode Add an Object Storage bucket Create an Access key Backblaze Provision B2 bucket Provision B2 app key","title":"Provision bucket"},{"location":"deployment/restic/#build-restic-environment","text":"Create a secure file to store needed environment variables for the restic client to read and write to the encrypted repository bucket: Linode #!/bin/bash RESTIC_REPOSITORY = \"s3:us-east-1.linodeobjects.com/myhost-restic\" RESTIC_PASSWORD = \"\" AWS_ACCESS_KEY_ID = \"\" # Access Key AWS_SECRET_ACCESS_KEY = \"\" # Secret Key Backblaze #!/bin/bash RESTIC_REPOSITORY = \"b2:myhost-restic\" RESTIC_PASSWORD = \"\" B2_ACCOUNT_ID = \"\" # keyID B2_ACCOUNT_KEY = \"\" # applicationKey Create /etc/restic.env from above template Tailor RESTIC_REPOSITORY to created bucket Generate RESTIC_PASSWORD and save to credential vault Fill storage credentials Secure configuration: sudo chmod 660 /etc/restic.env","title":"Build restic environment"},{"location":"deployment/restic/#initialize-repository","text":"Load the environment into your current shell to run Restic\u2019s one-time init command to set up the encrypted repository structure within the bucket: set -a ; source /etc/restic.env ; set +a hab pkg exec emergence/restic-snapshot restic init","title":"Initialize repository"},{"location":"deployment/restic/#create-backup-script","text":"Create /etc/cron.daily/emergence-restic-backup #!/bin/bash /bin/hab pkg exec emergence/restic-snapshot snapshot Max executable: chmod +x /etc/cron.daily/emergence-restic-backup","title":"Create backup script"},{"location":"deployment/restic/#run-manually","text":"To verify the configuration and create an initial snapshot: /etc/cron.daily/emergence-restic-backup","title":"Run manually"},{"location":"deployment/restic/#verifying-backups","text":"Load environment: set -a ; source /etc/restic.env ; set +a List snapshots: hab pkg exec emergence/restic-snapshot restic snapshots Examine contents of an SQL dump: hab pkg exec emergence/restic-snapshot restic dump ced0825f /database.sql | grep '^CREATE'","title":"Verifying backups"},{"location":"development/console-command/","text":"Development: Build a Console Command \u00b6 Create console-commands/myproject/mycommand.php Example content: <?php echo \"Hello \" . Site :: $title ; dump ( $_COMMAND ); $_COMMAND [ 'LOGGER' ] -> warning ( 'Danger Will Robinson!' ); Update site: update-site Run command: console-run myproject:mycommand --foo \"bar\"","title":"Development: Build a Console Command"},{"location":"development/console-command/#development-build-a-console-command","text":"Create console-commands/myproject/mycommand.php Example content: <?php echo \"Hello \" . Site :: $title ; dump ( $_COMMAND ); $_COMMAND [ 'LOGGER' ] -> warning ( 'Danger Will Robinson!' ); Update site: update-site Run command: console-run myproject:mycommand --foo \"bar\"","title":"Development: Build a Console Command"},{"location":"development/getting_started/","text":"Getting Started with Development \u00b6 This guide shows you the minimal steps required run an emergence project locally, make changes to it, and see the result. In this article \u00b6 Introduction Prerequisites Clone Project via Git Launch Studio via Habitat Start Runtime and Build Project Load Fixture Data (optional) Create User Account (optional) Running Tests Introduction \u00b6 Prerequisites \u00b6 Before you begin, you\u2019ll need Docker and Habitat set up on your workstation: Install Docker On Mac and Windows workstations, Docker must be installed to use habitat. On Linux, Docker is optional. - Download Docker for Mac - Download Docker for Windows - Install Docker on Ubuntu Install Chef Habitat on your system Chef Habitat is a tool for automating all the build and runtime workflows for applications, in a way that behaves consistently across time and environments. An application automated with Habitat can be run on any operating system, connected to other applications running locally or remotely, and deployed to either a container, virtual machine, or bare-metal system. Installing Habitat only adds one binary to your system, hab , and initializes the /hab tree. curl -s https://raw.githubusercontent.com/habitat-sh/habitat/master/components/hab/install.sh | sudo bash hab --version # should report 0.85.0 or newer Configure the hab client for your user Setting up Habitat will interactively ask questions to initialize ~/.hab . This command must be run once per user that will use hab : hab setup Clone Project via Git \u00b6 In this example, the slate-cbl project is cloned, but you might use any repository/branch containing an emergence project: git clone --recursive -b develop git@github.com:SlateFoundation/slate-cbl.git The --recursive option is used so that any submodule repositories are also cloned. Launch Studio via Habitat \u00b6 Change into project\u2019s cloned directory cd ./slate-cbl Launch Studio On any system, launch a studio with: HAB_DOCKER_OPTS = \"-p 7080:80 -p 3306:3306 --name emergence-studio\" \\ hab studio enter -D The HAB_DOCKER_OPTS environment variable here allows you to use any options supported by docker run , in this case forwarding ports for the web server and MySQL server from inside the container to your host machine. Review the notes printed to your terminal at the end of the studio startup process for a list of additional commands provided by your project\u2019s .studiorc Start Runtime and Build Site \u00b6 Ensure that the Supervisor is finished starting up in the background As documented in your terminal right before your studio prompt, the Habitat studio has launched a Supervisor for you and set up a shortcut command to follow its log. This log contains the output for the Supervisor and any background services we load into it. Your Supervisor should be done downloading and starting by the time you type anything, but you can check by running sup-log and ensure you see these last lines look like this: # sup-log ... hab-sup(MR): Starting gossip-listener on 0.0.0.0:9638 hab-sup(MR): Starting ctl-gateway on 127.0.0.1:9632 hab-sup(MR): Starting http-gateway on 0.0.0.0:9631 If you still see packages downloading and installing instead, wait until that finishes. Press Ctrl+C to stop following the log and return to your prompt. You can further confirm that the Supervisor is ready and see the status of any loaded services at any time by running hab svc status : # hab svc status No services loaded. Start environment services Use the Emergence Studio command start-all to launch and bind all the required services for loading an Emergence site: the http server (nginx), the application runtime (php-fpm+app bootloader), and a local mysql server: start-all At this point, you should be able to open localhost:7080 and see the error message Page not found . Build environment To build the entire environment and load it, use the studio command update-site : update-site At this point, localhost:7080 should display the current build of the site Load Fixture Data (optional) \u00b6 # clone fixture branch into git-ignored .data/ directory git clone -b cbl/competencies https://github.com/SlateFoundation/slate-fixtures.git .data/fixtures # load all .sql files from fixture cat .data/fixtures/*.sql | load-sql - Create User Account (optional) \u00b6 Enable user registration form (optional) If your project has registration disabled by default, you might want to enable it so you can register: # write class configuring enabling registration mkdir -p php-config/Emergence/People echo '<?php Emergence\\People\\RegistrationRequestHandler::$enableRegistration = true;' > php-config/Emergence/People/RegistrationRequestHandler.config.php # rebuild environment update-site Promote registered user to developer (optional) After visiting /register and creating a new user account, you can use the studio command promote-user to upgrade the user account you just registered to the highest access level: promote-user <myuser> After editing code in the working tree, run the studio command update-site to rebuild and update the environment. A watch-site command is also available to automatically rebuild and update the environment as changes are made to the working tree. Running Tests \u00b6 Cypress is used to provide browser-level full-stack testing. The package.json file at the root of the repository specifies the dependencies for running the test suite and all the configuration/tests for Cypress are container in the cypress/ tree at the root of the repository. To get started, from a terminal outside the studio in the root of the repository: # install development tooling locally npm install # launch cypress app npm run cypress:open","title":"Getting Started with Development"},{"location":"development/getting_started/#getting-started-with-development","text":"This guide shows you the minimal steps required run an emergence project locally, make changes to it, and see the result.","title":"Getting Started with Development"},{"location":"development/getting_started/#in-this-article","text":"Introduction Prerequisites Clone Project via Git Launch Studio via Habitat Start Runtime and Build Project Load Fixture Data (optional) Create User Account (optional) Running Tests","title":"In this article"},{"location":"development/getting_started/#introduction","text":"","title":"Introduction"},{"location":"development/getting_started/#prerequisites","text":"Before you begin, you\u2019ll need Docker and Habitat set up on your workstation: Install Docker On Mac and Windows workstations, Docker must be installed to use habitat. On Linux, Docker is optional. - Download Docker for Mac - Download Docker for Windows - Install Docker on Ubuntu Install Chef Habitat on your system Chef Habitat is a tool for automating all the build and runtime workflows for applications, in a way that behaves consistently across time and environments. An application automated with Habitat can be run on any operating system, connected to other applications running locally or remotely, and deployed to either a container, virtual machine, or bare-metal system. Installing Habitat only adds one binary to your system, hab , and initializes the /hab tree. curl -s https://raw.githubusercontent.com/habitat-sh/habitat/master/components/hab/install.sh | sudo bash hab --version # should report 0.85.0 or newer Configure the hab client for your user Setting up Habitat will interactively ask questions to initialize ~/.hab . This command must be run once per user that will use hab : hab setup","title":"Prerequisites"},{"location":"development/getting_started/#clone-project-via-git","text":"In this example, the slate-cbl project is cloned, but you might use any repository/branch containing an emergence project: git clone --recursive -b develop git@github.com:SlateFoundation/slate-cbl.git The --recursive option is used so that any submodule repositories are also cloned.","title":"Clone Project via Git"},{"location":"development/getting_started/#launch-studio-via-habitat","text":"Change into project\u2019s cloned directory cd ./slate-cbl Launch Studio On any system, launch a studio with: HAB_DOCKER_OPTS = \"-p 7080:80 -p 3306:3306 --name emergence-studio\" \\ hab studio enter -D The HAB_DOCKER_OPTS environment variable here allows you to use any options supported by docker run , in this case forwarding ports for the web server and MySQL server from inside the container to your host machine. Review the notes printed to your terminal at the end of the studio startup process for a list of additional commands provided by your project\u2019s .studiorc","title":"Launch Studio via Habitat"},{"location":"development/getting_started/#start-runtime-and-build-site","text":"Ensure that the Supervisor is finished starting up in the background As documented in your terminal right before your studio prompt, the Habitat studio has launched a Supervisor for you and set up a shortcut command to follow its log. This log contains the output for the Supervisor and any background services we load into it. Your Supervisor should be done downloading and starting by the time you type anything, but you can check by running sup-log and ensure you see these last lines look like this: # sup-log ... hab-sup(MR): Starting gossip-listener on 0.0.0.0:9638 hab-sup(MR): Starting ctl-gateway on 127.0.0.1:9632 hab-sup(MR): Starting http-gateway on 0.0.0.0:9631 If you still see packages downloading and installing instead, wait until that finishes. Press Ctrl+C to stop following the log and return to your prompt. You can further confirm that the Supervisor is ready and see the status of any loaded services at any time by running hab svc status : # hab svc status No services loaded. Start environment services Use the Emergence Studio command start-all to launch and bind all the required services for loading an Emergence site: the http server (nginx), the application runtime (php-fpm+app bootloader), and a local mysql server: start-all At this point, you should be able to open localhost:7080 and see the error message Page not found . Build environment To build the entire environment and load it, use the studio command update-site : update-site At this point, localhost:7080 should display the current build of the site","title":"Start Runtime and Build Site"},{"location":"development/getting_started/#load-fixture-data-optional","text":"# clone fixture branch into git-ignored .data/ directory git clone -b cbl/competencies https://github.com/SlateFoundation/slate-fixtures.git .data/fixtures # load all .sql files from fixture cat .data/fixtures/*.sql | load-sql -","title":"Load Fixture Data (optional)"},{"location":"development/getting_started/#create-user-account-optional","text":"Enable user registration form (optional) If your project has registration disabled by default, you might want to enable it so you can register: # write class configuring enabling registration mkdir -p php-config/Emergence/People echo '<?php Emergence\\People\\RegistrationRequestHandler::$enableRegistration = true;' > php-config/Emergence/People/RegistrationRequestHandler.config.php # rebuild environment update-site Promote registered user to developer (optional) After visiting /register and creating a new user account, you can use the studio command promote-user to upgrade the user account you just registered to the highest access level: promote-user <myuser> After editing code in the working tree, run the studio command update-site to rebuild and update the environment. A watch-site command is also available to automatically rebuild and update the environment as changes are made to the working tree.","title":"Create User Account (optional)"},{"location":"development/getting_started/#running-tests","text":"Cypress is used to provide browser-level full-stack testing. The package.json file at the root of the repository specifies the dependencies for running the test suite and all the configuration/tests for Cypress are container in the cypress/ tree at the root of the repository. To get started, from a terminal outside the studio in the root of the repository: # install development tooling locally npm install # launch cypress app npm run cypress:open","title":"Running Tests"},{"location":"development/site-specific-service/","text":"Development: Build a Site-specific Service \u00b6 This guide shows you how to develop a Chef Habitat package containing a complete build of one site that is ready to run as a service under the Chef Habitat Supervisor. In this article \u00b6 Introduction Initialize habitat/ tree Build runtime package Enter Studio Introduction \u00b6 After you complete this, you will be able to build a version of the site\u2019s source tree into a .hart Chef Habitat package build artifact file that is ready to be installed into any environment with the Chef Habitat Supervisor running, uploaded to a Chef Habitat Builder server, or exported into a Docker container image or other formats . The .hart file will contain a complete manifest of exact versions for all dependencies This approach also enables you to bundle any additional environmental dependencies, build steps, commands, configuration, or lifecycle hooks into your site\u2019s runtime environment via Chef Habitat\u2019s facilities. Initialize habitat/ Tree \u00b6 Before you begin, complete the steps covered in Deployment: Deploy a Site with Chef Habitat to initialize the habitat/ configuration tree defining the Chef Habitat package(s) for your site. Build Runtime Package \u00b6 Use the standard build command provided by Chef Habitat Studio as you would to build any other Chef Habitat package: cd \" ${ EMERGENCE_REPO } \" build Enter Studio \u00b6 A Chef Habitat Studio will provide an isolated, disposable, and interactive command-line environment for working on your package. When building your own Chef Habitat packages, be sure to set HAB_ORIGIN to the company/team/personal name you want to prefix your builds with. If you plan to publish your packages to a Chef Habitat Builder server, it should be an origin name you\u2019ve registered on that server. Otherwise the name is entirely for your own local organization and can be set to any string you could use for a directory name. Follow all steps in the Using Studio guide, but at the Load a Runtime Service step, pass the package identifier for your site-specific runtime: start-runtime myorigin/mypackage Update Site Without Rebuilding Runtime \u00b6 The most accurate way to test your site-specific service artifact is to use the build command to entirely rebuild the package each time. Once an instance of it is running though, you may update it directly from your site sources just like you would with the generic emergence/php-runtime service. Enable live-updating of site-specific service: enable-runtime-update Use update-site (or watch-site ) like normal: update-site","title":"Development: Build a Site-specific Service"},{"location":"development/site-specific-service/#development-build-a-site-specific-service","text":"This guide shows you how to develop a Chef Habitat package containing a complete build of one site that is ready to run as a service under the Chef Habitat Supervisor.","title":"Development: Build a Site-specific Service"},{"location":"development/site-specific-service/#in-this-article","text":"Introduction Initialize habitat/ tree Build runtime package Enter Studio","title":"In this article"},{"location":"development/site-specific-service/#introduction","text":"After you complete this, you will be able to build a version of the site\u2019s source tree into a .hart Chef Habitat package build artifact file that is ready to be installed into any environment with the Chef Habitat Supervisor running, uploaded to a Chef Habitat Builder server, or exported into a Docker container image or other formats . The .hart file will contain a complete manifest of exact versions for all dependencies This approach also enables you to bundle any additional environmental dependencies, build steps, commands, configuration, or lifecycle hooks into your site\u2019s runtime environment via Chef Habitat\u2019s facilities.","title":"Introduction"},{"location":"development/site-specific-service/#initialize-habitat-tree","text":"Before you begin, complete the steps covered in Deployment: Deploy a Site with Chef Habitat to initialize the habitat/ configuration tree defining the Chef Habitat package(s) for your site.","title":"Initialize habitat/ Tree"},{"location":"development/site-specific-service/#build-runtime-package","text":"Use the standard build command provided by Chef Habitat Studio as you would to build any other Chef Habitat package: cd \" ${ EMERGENCE_REPO } \" build","title":"Build Runtime Package"},{"location":"development/site-specific-service/#enter-studio","text":"A Chef Habitat Studio will provide an isolated, disposable, and interactive command-line environment for working on your package. When building your own Chef Habitat packages, be sure to set HAB_ORIGIN to the company/team/personal name you want to prefix your builds with. If you plan to publish your packages to a Chef Habitat Builder server, it should be an origin name you\u2019ve registered on that server. Otherwise the name is entirely for your own local organization and can be set to any string you could use for a directory name. Follow all steps in the Using Studio guide, but at the Load a Runtime Service step, pass the package identifier for your site-specific runtime: start-runtime myorigin/mypackage","title":"Enter Studio"},{"location":"development/site-specific-service/#update-site-without-rebuilding-runtime","text":"The most accurate way to test your site-specific service artifact is to use the build command to entirely rebuild the package each time. Once an instance of it is running though, you may update it directly from your site sources just like you would with the generic emergence/php-runtime service. Enable live-updating of site-specific service: enable-runtime-update Use update-site (or watch-site ) like normal: update-site","title":"Update Site Without Rebuilding Runtime"},{"location":"development/studio/","text":"Development: Using Studio \u00b6 This guide shows you how to set up a local Studio environment for interactively developing your site. In this article \u00b6 Introduction Prerequisites Add .studiorc to Repository Enter Studio Load a Database Service Load a Runtime Service Load an HTTP Service Studio Commands Reference Introduction \u00b6 Emergence Studio uses Chef Habitat Studio to provide an isolated, disposable, and interactive command-line environment for working on your site. In this and other guides, we will focus on running Linux-like Studio environments contained by Docker, as that provides the most consistent developer experience across platforms. On Linux machines, Chef Habitat also can provide a chroot-contained Studio type that is faster, smaller, persistent by default, and exposes all network ports. Chroot studios offer lower overhead in CI and controlled environments, and will work just as well with Emergence Studio, but require more awareness of what else is in the environment to avoid conflicts. Prerequisites \u00b6 Complete the prerequisites covered in Development: Getting Started Add .studiorc to Repository \u00b6 While entering a Chef Habitat Studio, any .studiorc script present in the root of the directory where the hab studio enter command is run will be sourced and executed automatically during the Studio\u2019s initialization, before the interactive command line is presented. Use this .studiorc template as a starting point in the root of any emergence site project: #!/bin/bash hab pkg install emergence/studio source \" $( hab pkg path emergence/studio ) /studio.sh\" This script installs and loads the Emergence Studio environment and commands. You may add any additional code here to extend the studio environment for your project. Enter Studio \u00b6 # start from the root of your project repository cd /path/to/my-repo/ # use your chosen/registered origin name export HAB_ORIGIN = \"myorigin\" # pass options to Docker to open ports, persist mysql data, name container export HAB_DOCKER_OPTS = \" -p 7080:80 -p 7043:443 -p 7036:3306 -p 7099:19999 -v mysite-mysql-data:/hab/svc/mysql/data --name mysite-studio \" # launch and enter interactive Habitat studio, forcing Docker mode hab studio enter -D Load a Database Service \u00b6 Start a database service before any other. Your runtime service instance can have its database slot bound to an instance of core/mysql , jarvus/mysql-remote , or any other Chef Habitat service with compatible config exposure leading to a MySQL server connection. Any approach must export $DB_SERVICE and $DB_DATABASE into your studio environment. Use core/mysql to run a local database \u00b6 start-mysql That\u2019s it! A local MySQL instance will be set up using core/mysql with persistent data stored under /hab/svc/mysql/data . The example HAB_DOCKER_OPTS above includes mounting this path to a named volume and exposing the MySQL port to your host machine on 7036 . Use hab svc status and sup-log to verify the database service is up and running without any errors. If you see any permission denied errors under sup-log , that likely indicates the volume mounted at /hab/svc/mysql/data did not initialize with permissions adequate for the MySQL service to write there, and can be fixed for the lifetime of that volume by running chown hab:hab -R /hab/svc/mysql/data Use jarvus/mysql-remote to connect a remote database \u00b6 start-mysql-remote \"mydatabasename\" This studio command will launch an editor to populate /hab/user/mysql-remote/config/user.toml with connection details and then load a jarvus/mysql-remote service instance. Load a Runtime Service \u00b6 With a database service successfully loaded, you can load a runtime service. The runtime service provides the PHP worker pool and loads your site code into it. start-runtime This studio command will load an instance of the generic emergence/php-runtime service that any site can be loaded into. The $DB_SERVICE environment variable exported when you ran one of the start-mysql* studio commands previously will be used to bind this runtime instance\u2019s database slot to your database instance. The guide Development: Build a Site-specific Service covers how to build and use your own runtime service package that can extend and bundle your site into its own deployable artifact. Any approach must export $EMERGENCE_RUNTIME into your studio environment. Load an HTTP Service \u00b6 Finally, with the database and runtime services successfully loaded, you can load an HTTP service to expose your application. start-http This studio command will load an instance of the generic emergence/nginx service. The $EMERGENCE_RUNTIME environment variable exported when you ran the start-runtime studio command previously will be used to bind this nginx instance\u2019s backend slot to your runtime instance. The example HAB_DOCKER_OPTS above includes exposing nginx\u2019s HTTP and HTTPS ports to your host machine on 7080 and 7043 respectively. Studio Commands Reference \u00b6 Runtime \u00b6 update-site \u2014 Projects your site tree with Hologit and loads it into the current runtime service instance Affected by $EMERGENCE_REPO , $EMERGENCE_HOLOBRANCH , and $EMERGENCE_FETCH environment variables watch-site \u2014 Like update-site , but runs until terminated, watching for file changes and re-updating the site automatically shell-runtime \u2014 Opens an interactive PsySH shell under the same environment used by the currently-loaded runtime instance switch-site /path/to/repository/ \u2014 Switches what site repository will be used next time you run update-site (the default is whichever .studiorc was loaded from) console-run [command] <args...> \u2014 Executes a script from the console-commands/ site tree within the current runtime enable-xdebug 192.168.1.1 \u2014 Enables using a remote Xdebug debugger at an IP/hostname reachable from inside the container enable-runtime-update \u2014 When using a site-specific runtime service, enables using update-site as you can with the generic emergence/php-runtime service (instead of using the site build bundled with the service). Database \u00b6 load-sql [-|file...|URL|site] [database] \u2014 Load one or more .sql files into the current database instance dump-sql [database] > file.sql \u2014 Dump the current database to SQL reset-mysql \u2014 Drop and recreate empty the current database promote-user <username> [account_level] \u2014 Promote a user in the database to a higher account level after registration shell-mysql \u2014 Open an interactive MySQL client shell under the currently-loaded database instance","title":"Development: Using Studio"},{"location":"development/studio/#development-using-studio","text":"This guide shows you how to set up a local Studio environment for interactively developing your site.","title":"Development: Using Studio"},{"location":"development/studio/#in-this-article","text":"Introduction Prerequisites Add .studiorc to Repository Enter Studio Load a Database Service Load a Runtime Service Load an HTTP Service Studio Commands Reference","title":"In this article"},{"location":"development/studio/#introduction","text":"Emergence Studio uses Chef Habitat Studio to provide an isolated, disposable, and interactive command-line environment for working on your site. In this and other guides, we will focus on running Linux-like Studio environments contained by Docker, as that provides the most consistent developer experience across platforms. On Linux machines, Chef Habitat also can provide a chroot-contained Studio type that is faster, smaller, persistent by default, and exposes all network ports. Chroot studios offer lower overhead in CI and controlled environments, and will work just as well with Emergence Studio, but require more awareness of what else is in the environment to avoid conflicts.","title":"Introduction"},{"location":"development/studio/#prerequisites","text":"Complete the prerequisites covered in Development: Getting Started","title":"Prerequisites"},{"location":"development/studio/#add-studiorc-to-repository","text":"While entering a Chef Habitat Studio, any .studiorc script present in the root of the directory where the hab studio enter command is run will be sourced and executed automatically during the Studio\u2019s initialization, before the interactive command line is presented. Use this .studiorc template as a starting point in the root of any emergence site project: #!/bin/bash hab pkg install emergence/studio source \" $( hab pkg path emergence/studio ) /studio.sh\" This script installs and loads the Emergence Studio environment and commands. You may add any additional code here to extend the studio environment for your project.","title":"Add .studiorc to Repository"},{"location":"development/studio/#enter-studio","text":"# start from the root of your project repository cd /path/to/my-repo/ # use your chosen/registered origin name export HAB_ORIGIN = \"myorigin\" # pass options to Docker to open ports, persist mysql data, name container export HAB_DOCKER_OPTS = \" -p 7080:80 -p 7043:443 -p 7036:3306 -p 7099:19999 -v mysite-mysql-data:/hab/svc/mysql/data --name mysite-studio \" # launch and enter interactive Habitat studio, forcing Docker mode hab studio enter -D","title":"Enter Studio"},{"location":"development/studio/#load-a-database-service","text":"Start a database service before any other. Your runtime service instance can have its database slot bound to an instance of core/mysql , jarvus/mysql-remote , or any other Chef Habitat service with compatible config exposure leading to a MySQL server connection. Any approach must export $DB_SERVICE and $DB_DATABASE into your studio environment.","title":"Load a Database Service"},{"location":"development/studio/#use-coremysql-to-run-a-local-database","text":"start-mysql That\u2019s it! A local MySQL instance will be set up using core/mysql with persistent data stored under /hab/svc/mysql/data . The example HAB_DOCKER_OPTS above includes mounting this path to a named volume and exposing the MySQL port to your host machine on 7036 . Use hab svc status and sup-log to verify the database service is up and running without any errors. If you see any permission denied errors under sup-log , that likely indicates the volume mounted at /hab/svc/mysql/data did not initialize with permissions adequate for the MySQL service to write there, and can be fixed for the lifetime of that volume by running chown hab:hab -R /hab/svc/mysql/data","title":"Use core/mysql to run a local database"},{"location":"development/studio/#use-jarvusmysql-remote-to-connect-a-remote-database","text":"start-mysql-remote \"mydatabasename\" This studio command will launch an editor to populate /hab/user/mysql-remote/config/user.toml with connection details and then load a jarvus/mysql-remote service instance.","title":"Use jarvus/mysql-remote to connect a remote database"},{"location":"development/studio/#load-a-runtime-service","text":"With a database service successfully loaded, you can load a runtime service. The runtime service provides the PHP worker pool and loads your site code into it. start-runtime This studio command will load an instance of the generic emergence/php-runtime service that any site can be loaded into. The $DB_SERVICE environment variable exported when you ran one of the start-mysql* studio commands previously will be used to bind this runtime instance\u2019s database slot to your database instance. The guide Development: Build a Site-specific Service covers how to build and use your own runtime service package that can extend and bundle your site into its own deployable artifact. Any approach must export $EMERGENCE_RUNTIME into your studio environment.","title":"Load a Runtime Service"},{"location":"development/studio/#load-an-http-service","text":"Finally, with the database and runtime services successfully loaded, you can load an HTTP service to expose your application. start-http This studio command will load an instance of the generic emergence/nginx service. The $EMERGENCE_RUNTIME environment variable exported when you ran the start-runtime studio command previously will be used to bind this nginx instance\u2019s backend slot to your runtime instance. The example HAB_DOCKER_OPTS above includes exposing nginx\u2019s HTTP and HTTPS ports to your host machine on 7080 and 7043 respectively.","title":"Load an HTTP Service"},{"location":"development/studio/#studio-commands-reference","text":"","title":"Studio Commands Reference"},{"location":"development/studio/#runtime","text":"update-site \u2014 Projects your site tree with Hologit and loads it into the current runtime service instance Affected by $EMERGENCE_REPO , $EMERGENCE_HOLOBRANCH , and $EMERGENCE_FETCH environment variables watch-site \u2014 Like update-site , but runs until terminated, watching for file changes and re-updating the site automatically shell-runtime \u2014 Opens an interactive PsySH shell under the same environment used by the currently-loaded runtime instance switch-site /path/to/repository/ \u2014 Switches what site repository will be used next time you run update-site (the default is whichever .studiorc was loaded from) console-run [command] <args...> \u2014 Executes a script from the console-commands/ site tree within the current runtime enable-xdebug 192.168.1.1 \u2014 Enables using a remote Xdebug debugger at an IP/hostname reachable from inside the container enable-runtime-update \u2014 When using a site-specific runtime service, enables using update-site as you can with the generic emergence/php-runtime service (instead of using the site build bundled with the service).","title":"Runtime"},{"location":"development/studio/#database","text":"load-sql [-|file...|URL|site] [database] \u2014 Load one or more .sql files into the current database instance dump-sql [database] > file.sql \u2014 Dump the current database to SQL reset-mysql \u2014 Drop and recreate empty the current database promote-user <username> [account_level] \u2014 Promote a user in the database to a higher account level after registration shell-mysql \u2014 Open an interactive MySQL client shell under the currently-loaded database instance","title":"Database"},{"location":"guides/creating-a-dynamic-page/","text":"Creating a Dynamic Page \u00b6 <?php print ( \"Hello world!\" ); Why the <?php ? So you can do this: <html> <body> <p> <?php print ( \"Hello, it&rsquo;s \" . date ( 'Y-m-d' )); ?> </p> </body> </html> There are syntax shortcuts available for something so simple though: <html> <body> <p>Hello, it&rsquo;s <? = date ( 'Y-m-d' ) ?> </p> </body> </html>","title":"Creating a Dynamic Page"},{"location":"guides/creating-a-dynamic-page/#creating-a-dynamic-page","text":"<?php print ( \"Hello world!\" ); Why the <?php ? So you can do this: <html> <body> <p> <?php print ( \"Hello, it&rsquo;s \" . date ( 'Y-m-d' )); ?> </p> </body> </html> There are syntax shortcuts available for something so simple though: <html> <body> <p>Hello, it&rsquo;s <? = date ( 'Y-m-d' ) ?> </p> </body> </html>","title":"Creating a Dynamic Page"},{"location":"guides/make-a-model-searchable-by-tags/","text":"Make a Model Searchable by Tags \u00b6 To make a model searchable by tags, we\u2019ll make use of the callback option for $searchConditions to specify a function that can generate advanced SQL procedurally. First, define a new search condition: <?php class MyModel extends \\ActiveRecord { // ... public static $searchConditions = [ 'Tags' => [ 'qualifiers' => [ 'any' , 'tag' , 'tags' ], 'points' => 2 , 'callback' => 'getTagsSearchConditions' ] ]; // ... } The callback option can take any PHP callable , or a simple string referencing a public static method on the model class. This method will be called for each term in the search query and should return an SQL expression to match records that match this term. The ultimate results list gets sorted by how many terms are matched. While it is possible to implement joins via $searchConditions , in this example we are instead going to \u201cunroll\u201d any matching tags ahead of time into a list of tagged record IDs while the search query is being prepared: public static function getTagsSearchConditions($term) { $ids = \\DB::allValues( 'ContextID', ' SELECT TagItem.ContextID FROM `%1$s` Tag JOIN `%2$s` TagItem ON TagItem.TagID = Tag.ID AND TagItem.ContextClass = \"%3$s\" WHERE Tag.Title LIKE \"%%%4$s%%\" OR Tag.Handle LIKE \"%%%4$s%%\" ', [ \\Tag::$tableName, // %1$s \\TagItem::$tableName, // %2$s \\DB::escape(static::getStaticRootClass()), // %3$s \\DB::escape($term) // %4$s ] ); return count($ids) ? 'ID IN ('.implode(',',$ids).')' : '0'; }","title":"Make a Model Searchable by Tags"},{"location":"guides/make-a-model-searchable-by-tags/#make-a-model-searchable-by-tags","text":"To make a model searchable by tags, we\u2019ll make use of the callback option for $searchConditions to specify a function that can generate advanced SQL procedurally. First, define a new search condition: <?php class MyModel extends \\ActiveRecord { // ... public static $searchConditions = [ 'Tags' => [ 'qualifiers' => [ 'any' , 'tag' , 'tags' ], 'points' => 2 , 'callback' => 'getTagsSearchConditions' ] ]; // ... } The callback option can take any PHP callable , or a simple string referencing a public static method on the model class. This method will be called for each term in the search query and should return an SQL expression to match records that match this term. The ultimate results list gets sorted by how many terms are matched. While it is possible to implement joins via $searchConditions , in this example we are instead going to \u201cunroll\u201d any matching tags ahead of time into a list of tagged record IDs while the search query is being prepared: public static function getTagsSearchConditions($term) { $ids = \\DB::allValues( 'ContextID', ' SELECT TagItem.ContextID FROM `%1$s` Tag JOIN `%2$s` TagItem ON TagItem.TagID = Tag.ID AND TagItem.ContextClass = \"%3$s\" WHERE Tag.Title LIKE \"%%%4$s%%\" OR Tag.Handle LIKE \"%%%4$s%%\" ', [ \\Tag::$tableName, // %1$s \\TagItem::$tableName, // %2$s \\DB::escape(static::getStaticRootClass()), // %3$s \\DB::escape($term) // %4$s ] ); return count($ids) ? 'ID IN ('.implode(',',$ids).')' : '0'; }","title":"Make a Model Searchable by Tags"},{"location":"guides/migrate-site-to-holobranch/","text":"Migrate a Site to a Holobranch \u00b6 Branch name \u00b6 For this guide, the branch name emergence/vfs-site/v1 will be used as the target for projection, but any valid git branch name could be used. This example follows a convention of using the emergence/vfs-site/ prefix for holobranch projections intended to be loaded into VFS-powered Emergence sites. So a branch named emergence/vfs-site/v1 might indicate VFS-compatible site projections from a v1.x release stream, while emergence/vfs-site/develop might be used to indicate such from a develop branch. Initialize projected branch with a VFS snapshot (optional) \u00b6 When a new holobranch is projected to a branch for the first time, an empty initialization commit will be generated automatically. Optionally, you can manually initialize the branch you\u2019ll project to with a snapshot of the existing VFS composite first. Full history via git gateway \u00b6 This relies on the .git gateway merged into skeleton-v1 v1.11.0 , so the target site may need to be updated via traditional VFS means first to acquire the latest code in skeleton-v1/skeleton-v2. By initializing your projected branch this way, you\u2019ll be able to capture and review the effective set of tree changes that happen in the switchover. An issue with the git gateway currently prevents adding it as a new remote in an existing repo, but cloning into a fresh repo works: cd .. git clone http://my-site.example.org/.git Then from that local clone, you can fetch the composite master branch as the start of your projection target: cd ./my-repo git fetch ../my-site.example.org master:emergence/vfs-site/v1 Remote snapshot via WebDAV \u00b6 If the git gateway is unavailable on the target site, or a history is not needed, a complete snapshot can be captured remotely over the standard /develop interface using the @emergence/source-http-legacy NPM package: # install command globally npm install -g @emergence/source-http-legacy # build snapshot tree of entire VFS emergence-source-http-legacy pull my-site.example.org # using tree hash output at end of pull command TREE_HASH = 4b825dc642cb6eb9a060e54bf8d69288fbee4904 git commit-tree -m \"snapshot mysite.example.org\" \" ${ TREE_HASH } \" # using commit hash output at end of commit-tree command COMMIT_HASH = 4b825dc642cb6eb9a060e54bf8d69288fbee4904 git update-ref refs/heads/emergence/vfs-site/v1 Update git mappings to include all trees \u00b6 Create or update a php-config/Git.config.d/* file mapping the VFS to projected git branch: <?php Git :: $repositories [ 'my-repo' ] = [ 'remote' => 'git@github.com:my-org/my-repo.git' , 'originBranch' => 'emergence/vfs-site/v1' , 'workingBranch' => 'emergence/vfs-site/v1' , 'trees' => [ 'api-docs' , 'console-commands' , 'cypress' , 'data-exporters' , 'dwoo-plugins' , 'event-handlers' , 'html-templates' , 'php-classes' , 'php-config' , 'php-migrations' , 'phpunit-tests' , // 'sencha-workspace', 'site-root' , 'site-tasks' , 'webapp-builds' , // 'webapp-plugin-builds', ] ]; Project holobranch and push to server \u00b6 git holo project --fetch = '*' emergence-vfs-site --commit-to = emergence/vfs-site/v1 git push origin emergence/vfs-site/v1 Reconfigure site \u00b6 You could edit the site.json file directly and then restart the kernel to reconfigure the site, or use an HTTP client like HTTPie to make the changes via the kernel API: # install HTTPie command sudo hab pkg install core/python sudo hab pkg exec core/python pip install httpie httpie-unixsocket sudo hab pkg binlink core/python http # verify connection to kernel API and site selection export SITE_HANDLE = \"my-site\" sudo http \\ GET http+unix://%2Femergence%2Fkernel.sock/sites/ ${ SITE_HANDLE } # null out parent_hostname and parent_key sudo http \\ PATCH http+unix://%2Femergence%2Fkernel.sock/sites/ ${ SITE_HANDLE } \\ parent_hostname: = null \\ parent_key: = null Purge parent cache \u00b6 Open MySQL shell for the site: emergence-mysql-shell ${ SITE_HANDLE } Then run the following queries: DELETE FROM _e_files WHERE CollectionID IN ( SELECT ID FROM _e_file_collections WHERE Site = \"Remote\" ); DELETE FROM _e_file_collections WHERE Site = \"Remote\" ; Import vfs-site branch \u00b6 Switch site repo to new vfs-site branch: export SOURCE_NAME = \"mysourcename\" sudo emergence-git-shell ${ SITE_HANDLE } ${ SOURCE_NAME } git fetch --all git checkout emergence/vfs-site/v1 pwd exit Change into that directory and open a PHP shell: cd \"/emergence/sites/ ${ SITE_HANDLE } /site-data/git/ ${ SOURCE_NAME } \" emergence-shell ${ SITE_HANDLE } Then run the following PHP code: $collections = array_diff(glob('*'), ['sencha-workspace']); foreach ($collections as $collection) { echo \"Importing $collection\\n\\t\"; echo http_build_query(Emergence_FS::importTree($collection, $collection)); echo \"\\n\\n\"; } echo NestingBehavior::repairTable(SiteCollection::class, 'PosLeft', 'PosRight').\" collections renested\\n\\n\"; Then stop/start PHP to clear all caches and run repair on the VFS, and then stop/start PHP again for good measure","title":"Migrate a Site to a Holobranch"},{"location":"guides/migrate-site-to-holobranch/#migrate-a-site-to-a-holobranch","text":"","title":"Migrate a Site to a Holobranch"},{"location":"guides/migrate-site-to-holobranch/#branch-name","text":"For this guide, the branch name emergence/vfs-site/v1 will be used as the target for projection, but any valid git branch name could be used. This example follows a convention of using the emergence/vfs-site/ prefix for holobranch projections intended to be loaded into VFS-powered Emergence sites. So a branch named emergence/vfs-site/v1 might indicate VFS-compatible site projections from a v1.x release stream, while emergence/vfs-site/develop might be used to indicate such from a develop branch.","title":"Branch name"},{"location":"guides/migrate-site-to-holobranch/#initialize-projected-branch-with-a-vfs-snapshot-optional","text":"When a new holobranch is projected to a branch for the first time, an empty initialization commit will be generated automatically. Optionally, you can manually initialize the branch you\u2019ll project to with a snapshot of the existing VFS composite first.","title":"Initialize projected branch with a VFS snapshot (optional)"},{"location":"guides/migrate-site-to-holobranch/#full-history-via-git-gateway","text":"This relies on the .git gateway merged into skeleton-v1 v1.11.0 , so the target site may need to be updated via traditional VFS means first to acquire the latest code in skeleton-v1/skeleton-v2. By initializing your projected branch this way, you\u2019ll be able to capture and review the effective set of tree changes that happen in the switchover. An issue with the git gateway currently prevents adding it as a new remote in an existing repo, but cloning into a fresh repo works: cd .. git clone http://my-site.example.org/.git Then from that local clone, you can fetch the composite master branch as the start of your projection target: cd ./my-repo git fetch ../my-site.example.org master:emergence/vfs-site/v1","title":"Full history via git gateway"},{"location":"guides/migrate-site-to-holobranch/#remote-snapshot-via-webdav","text":"If the git gateway is unavailable on the target site, or a history is not needed, a complete snapshot can be captured remotely over the standard /develop interface using the @emergence/source-http-legacy NPM package: # install command globally npm install -g @emergence/source-http-legacy # build snapshot tree of entire VFS emergence-source-http-legacy pull my-site.example.org # using tree hash output at end of pull command TREE_HASH = 4b825dc642cb6eb9a060e54bf8d69288fbee4904 git commit-tree -m \"snapshot mysite.example.org\" \" ${ TREE_HASH } \" # using commit hash output at end of commit-tree command COMMIT_HASH = 4b825dc642cb6eb9a060e54bf8d69288fbee4904 git update-ref refs/heads/emergence/vfs-site/v1","title":"Remote snapshot via WebDAV"},{"location":"guides/migrate-site-to-holobranch/#update-git-mappings-to-include-all-trees","text":"Create or update a php-config/Git.config.d/* file mapping the VFS to projected git branch: <?php Git :: $repositories [ 'my-repo' ] = [ 'remote' => 'git@github.com:my-org/my-repo.git' , 'originBranch' => 'emergence/vfs-site/v1' , 'workingBranch' => 'emergence/vfs-site/v1' , 'trees' => [ 'api-docs' , 'console-commands' , 'cypress' , 'data-exporters' , 'dwoo-plugins' , 'event-handlers' , 'html-templates' , 'php-classes' , 'php-config' , 'php-migrations' , 'phpunit-tests' , // 'sencha-workspace', 'site-root' , 'site-tasks' , 'webapp-builds' , // 'webapp-plugin-builds', ] ];","title":"Update git mappings to include all trees"},{"location":"guides/migrate-site-to-holobranch/#project-holobranch-and-push-to-server","text":"git holo project --fetch = '*' emergence-vfs-site --commit-to = emergence/vfs-site/v1 git push origin emergence/vfs-site/v1","title":"Project holobranch and push to server"},{"location":"guides/migrate-site-to-holobranch/#reconfigure-site","text":"You could edit the site.json file directly and then restart the kernel to reconfigure the site, or use an HTTP client like HTTPie to make the changes via the kernel API: # install HTTPie command sudo hab pkg install core/python sudo hab pkg exec core/python pip install httpie httpie-unixsocket sudo hab pkg binlink core/python http # verify connection to kernel API and site selection export SITE_HANDLE = \"my-site\" sudo http \\ GET http+unix://%2Femergence%2Fkernel.sock/sites/ ${ SITE_HANDLE } # null out parent_hostname and parent_key sudo http \\ PATCH http+unix://%2Femergence%2Fkernel.sock/sites/ ${ SITE_HANDLE } \\ parent_hostname: = null \\ parent_key: = null","title":"Reconfigure site"},{"location":"guides/migrate-site-to-holobranch/#purge-parent-cache","text":"Open MySQL shell for the site: emergence-mysql-shell ${ SITE_HANDLE } Then run the following queries: DELETE FROM _e_files WHERE CollectionID IN ( SELECT ID FROM _e_file_collections WHERE Site = \"Remote\" ); DELETE FROM _e_file_collections WHERE Site = \"Remote\" ;","title":"Purge parent cache"},{"location":"guides/migrate-site-to-holobranch/#import-vfs-site-branch","text":"Switch site repo to new vfs-site branch: export SOURCE_NAME = \"mysourcename\" sudo emergence-git-shell ${ SITE_HANDLE } ${ SOURCE_NAME } git fetch --all git checkout emergence/vfs-site/v1 pwd exit Change into that directory and open a PHP shell: cd \"/emergence/sites/ ${ SITE_HANDLE } /site-data/git/ ${ SOURCE_NAME } \" emergence-shell ${ SITE_HANDLE } Then run the following PHP code: $collections = array_diff(glob('*'), ['sencha-workspace']); foreach ($collections as $collection) { echo \"Importing $collection\\n\\t\"; echo http_build_query(Emergence_FS::importTree($collection, $collection)); echo \"\\n\\n\"; } echo NestingBehavior::repairTable(SiteCollection::class, 'PosLeft', 'PosRight').\" collections renested\\n\\n\"; Then stop/start PHP to clear all caches and run repair on the VFS, and then stop/start PHP again for good measure","title":"Import vfs-site branch"},{"location":"guides/migrations/","text":"Migrations \u00b6 Migrations are scripts designed to run once on each instance they\u2019re deployed to in the order they were created. They are most useful for transforming database structure but can also be used to check or update configuration or other application customizations. Migrations state storage \u00b6 The status of each migration script is tracked in the _e_migrations table. New scripts have no record in the table and are considered pending and in need of execution. Once started, a migration script cannot be run again without manually resetting its status in the _e_migrations table. Developer user interface \u00b6 A user interface is available for Developer+ users online at /site-admin/migrations . From here you can view a list of all pending , started , skipped , failed , and executed migrations and execute any that are pending. Philosophy \u00b6 Migration scripts in emergence don\u2019t have the usually up / down workflow that migration frameworks usually provide. Instead, it is expected that a website/database be snapshotted before migrations are run as this provides a more reliable recovery process in the case of failed migrations. Each script implements only one routine: upgrade if needed. The goal of every migration script is to first check if it can do nothing and return the status skipped , and then execute the migration and return the status executed or failed . A migration should be safe to run multiple times and return skipped when it can test that it has been run already. Migrations should make incremental changes, only changing that which it checked for, such as adding a database column or changing its type. Table schemas can be generated in a number of ways and can be augmented by configuration from other packages, so moving a table to a total known state isn\u2019t the goal. Building a migration \u00b6 Creating script file \u00b6 Create a new file under the php-migrations tree, placing it under a folder reflecting the highest common PHP namespace of the application/database elements it affects. Each file starts with a datestamp in the format YYYYMMDD which is used to order the execution of migrations. The datestame can optionally further include a time component in the format YYYYMMDDHHMMSS to order the execution of migrations added on the same day. Many older migrations filled the time component with 000000 but this can be ommitted now with the same effect. Script scope \u00b6 Each migration is executed in a closed scope with three variables predefined: $migration : An array containing the stored metadata about the transaction $migrationNode : A SiteFile instance for the current script being executed $resetMigrationStatus : A callable you can execute to delete the stored status for the current migration (useful while debugging, see below) Additionally, the migration runs in the scope of a member of the Emergence\\SiteAdmin\\MigrationsRequestHandler class and has access to a number of protected static methods it provides: static::tableExists($tableName) static::columnExists($tableName, $columnName) static::getColumns($tableName) static::getColumnNames($tableName) static::getColumn($tableName, $columnName) static::getColumnType($tableName, $columnName) static::getColumnKey($tableName, $columnName) static::getColumnDefault($tableName, $columnName) static::getColumnIsNullable($tableName, $columnName) static::getConstraints($tableName) static::getConstraint($tableName, $constraintName) static::addColumn($tableName, $columnName, $definition, $position = null) static::addIndex($tableName, $indexName, array $columns = [], $type = null) static::dropColumn($tableName, $columnName) All output is captured and reported on after a migration is executed but not (currently) saved. Debugging \u00b6 The best workflow for debugging a migration is to dump and reload all application tables (including _e_migrations but excluding _e_file* VFS tables if present) between each execution. During the development though, you might find it helpful to call $resetMigrationStatus() at the beggining of your script or return static::STATUS_DEBUG to erase the _e_migrations record that would prevent you from running it over and over again. Example migrations \u00b6 Add a column \u00b6 This migration from slate-cbl is about as simple as it gets: <?php namespace Slate\\CBL\\Tasks ; // skip if Task table does not exist or already has ClonedTaskID if ( ! static :: tableExists ( Task :: $tableName )) { printf ( \"Skipping migration because table `%s` does not yet exist \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } if ( static :: columnExists ( Task :: $tableName , 'ClonedTaskID' )) { printf ( \"Skipping migration because column `%s`.`ClonedTaskID` already exists \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } // add ClonedTaskID column to Task table static :: addColumn ( Task :: $tableName , 'ClonedTaskID' , 'int unsigned NULL default NULL' , 'AFTER `ParentTaskID`' ); // finish return static :: STATUS_EXECUTED ; Move column to parent record \u00b6 This migration, also from slate-cbl , is about as complex as it gets: <?php namespace Slate\\CBL\\Tasks ; use DB ; use HandleBehavior ; // skip if Task table does not exist or already has SectionID if ( ! static :: tableExists ( Task :: $tableName )) { printf ( \"Skipping migration because table `%s` does not yet exist \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } if ( static :: columnExists ( Task :: $tableName , 'SectionID' )) { printf ( \"Skipping migration because column `%s`.`SectionID` already exists \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } // find existing SectionIDs for all associated StudentTask records, clone Task records as needed $taskColumnNames = array_diff ( static :: getColumnNames ( Task :: $tableName ), [ 'ID' , 'Handle' ]); $taskSectionIds = DB :: arrayTable ( 'TaskID' , 'SELECT DISTINCT TaskID, SectionID FROM `%s`' , StudentTask :: $tableName ); $taskSectionId = []; foreach ( $taskSectionIds as $taskId => $studentTaskSections ) { // original task gets first section if ( $studentTaskSection = array_shift ( $studentTaskSections )) { $taskSectionId [ $taskId ] = $studentTaskSection [ 'SectionID' ]; } // no cloning is needed if ( count ( $studentTaskSections ) == 0 ) { continue ; } $taskTitle = DB :: oneValue ( 'SELECT Title FROM `%s` WHERE ID = %u' , [ Task :: $tableName , $taskId ]); // clone for any/each additional section while ( $studentTaskSection = array_shift ( $studentTaskSections )) { $cloneHandle = HandleBehavior :: getUniqueHandle ( Task :: class , $taskTitle ); DB :: nonQuery ( 'INSERT INTO `%1$s` (`ID`, `Handle`, `%4$s`) SELECT NULL, \"%3$s\", `%4$s` FROM `%1$s` WHERE ID = %2$u' , [ Task :: $tableName , $taskId , DB :: escape ( $cloneHandle ), implode ( '`, `' , $taskColumnNames ) ] ); $taskSectionId [ DB :: insertID ()] = $studentTaskSection [ 'SectionID' ]; printf ( \"Cloning task %u for section %u \\n \" , $taskId , $studentTaskSection [ 'SectionID' ]); } } // add SectionID column to Task table static :: addColumn ( Task :: $tableName , 'SectionID' , 'int unsigned NULL default NULL' , 'AFTER `ModifierID`' ); static :: addIndex ( Task :: $tableName , 'SectionID' ); // clone Task for each SectionID where multiple are associated printf ( \"Setting SectionID for %u Task records \\n \" , count ( $taskSectionId )); foreach ( $taskSectionId as $taskId => $sectionId ) { DB :: nonQuery ( 'UPDATE `%s` SET SectionID = %u WHERE ID = %u' , [ Task :: $tableName , $sectionId , $taskId ]); } // remove SectionID column from StudentTask table static :: dropColumn ( StudentTask :: $tableName , 'SectionID' ); // finish return static :: STATUS_EXECUTED ;","title":"Migrations"},{"location":"guides/migrations/#migrations","text":"Migrations are scripts designed to run once on each instance they\u2019re deployed to in the order they were created. They are most useful for transforming database structure but can also be used to check or update configuration or other application customizations.","title":"Migrations"},{"location":"guides/migrations/#migrations-state-storage","text":"The status of each migration script is tracked in the _e_migrations table. New scripts have no record in the table and are considered pending and in need of execution. Once started, a migration script cannot be run again without manually resetting its status in the _e_migrations table.","title":"Migrations state storage"},{"location":"guides/migrations/#developer-user-interface","text":"A user interface is available for Developer+ users online at /site-admin/migrations . From here you can view a list of all pending , started , skipped , failed , and executed migrations and execute any that are pending.","title":"Developer user interface"},{"location":"guides/migrations/#philosophy","text":"Migration scripts in emergence don\u2019t have the usually up / down workflow that migration frameworks usually provide. Instead, it is expected that a website/database be snapshotted before migrations are run as this provides a more reliable recovery process in the case of failed migrations. Each script implements only one routine: upgrade if needed. The goal of every migration script is to first check if it can do nothing and return the status skipped , and then execute the migration and return the status executed or failed . A migration should be safe to run multiple times and return skipped when it can test that it has been run already. Migrations should make incremental changes, only changing that which it checked for, such as adding a database column or changing its type. Table schemas can be generated in a number of ways and can be augmented by configuration from other packages, so moving a table to a total known state isn\u2019t the goal.","title":"Philosophy"},{"location":"guides/migrations/#building-a-migration","text":"","title":"Building a migration"},{"location":"guides/migrations/#creating-script-file","text":"Create a new file under the php-migrations tree, placing it under a folder reflecting the highest common PHP namespace of the application/database elements it affects. Each file starts with a datestamp in the format YYYYMMDD which is used to order the execution of migrations. The datestame can optionally further include a time component in the format YYYYMMDDHHMMSS to order the execution of migrations added on the same day. Many older migrations filled the time component with 000000 but this can be ommitted now with the same effect.","title":"Creating script file"},{"location":"guides/migrations/#script-scope","text":"Each migration is executed in a closed scope with three variables predefined: $migration : An array containing the stored metadata about the transaction $migrationNode : A SiteFile instance for the current script being executed $resetMigrationStatus : A callable you can execute to delete the stored status for the current migration (useful while debugging, see below) Additionally, the migration runs in the scope of a member of the Emergence\\SiteAdmin\\MigrationsRequestHandler class and has access to a number of protected static methods it provides: static::tableExists($tableName) static::columnExists($tableName, $columnName) static::getColumns($tableName) static::getColumnNames($tableName) static::getColumn($tableName, $columnName) static::getColumnType($tableName, $columnName) static::getColumnKey($tableName, $columnName) static::getColumnDefault($tableName, $columnName) static::getColumnIsNullable($tableName, $columnName) static::getConstraints($tableName) static::getConstraint($tableName, $constraintName) static::addColumn($tableName, $columnName, $definition, $position = null) static::addIndex($tableName, $indexName, array $columns = [], $type = null) static::dropColumn($tableName, $columnName) All output is captured and reported on after a migration is executed but not (currently) saved.","title":"Script scope"},{"location":"guides/migrations/#debugging","text":"The best workflow for debugging a migration is to dump and reload all application tables (including _e_migrations but excluding _e_file* VFS tables if present) between each execution. During the development though, you might find it helpful to call $resetMigrationStatus() at the beggining of your script or return static::STATUS_DEBUG to erase the _e_migrations record that would prevent you from running it over and over again.","title":"Debugging"},{"location":"guides/migrations/#example-migrations","text":"","title":"Example migrations"},{"location":"guides/migrations/#add-a-column","text":"This migration from slate-cbl is about as simple as it gets: <?php namespace Slate\\CBL\\Tasks ; // skip if Task table does not exist or already has ClonedTaskID if ( ! static :: tableExists ( Task :: $tableName )) { printf ( \"Skipping migration because table `%s` does not yet exist \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } if ( static :: columnExists ( Task :: $tableName , 'ClonedTaskID' )) { printf ( \"Skipping migration because column `%s`.`ClonedTaskID` already exists \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } // add ClonedTaskID column to Task table static :: addColumn ( Task :: $tableName , 'ClonedTaskID' , 'int unsigned NULL default NULL' , 'AFTER `ParentTaskID`' ); // finish return static :: STATUS_EXECUTED ;","title":"Add a column"},{"location":"guides/migrations/#move-column-to-parent-record","text":"This migration, also from slate-cbl , is about as complex as it gets: <?php namespace Slate\\CBL\\Tasks ; use DB ; use HandleBehavior ; // skip if Task table does not exist or already has SectionID if ( ! static :: tableExists ( Task :: $tableName )) { printf ( \"Skipping migration because table `%s` does not yet exist \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } if ( static :: columnExists ( Task :: $tableName , 'SectionID' )) { printf ( \"Skipping migration because column `%s`.`SectionID` already exists \\n \" , Task :: $tableName ); return static :: STATUS_SKIPPED ; } // find existing SectionIDs for all associated StudentTask records, clone Task records as needed $taskColumnNames = array_diff ( static :: getColumnNames ( Task :: $tableName ), [ 'ID' , 'Handle' ]); $taskSectionIds = DB :: arrayTable ( 'TaskID' , 'SELECT DISTINCT TaskID, SectionID FROM `%s`' , StudentTask :: $tableName ); $taskSectionId = []; foreach ( $taskSectionIds as $taskId => $studentTaskSections ) { // original task gets first section if ( $studentTaskSection = array_shift ( $studentTaskSections )) { $taskSectionId [ $taskId ] = $studentTaskSection [ 'SectionID' ]; } // no cloning is needed if ( count ( $studentTaskSections ) == 0 ) { continue ; } $taskTitle = DB :: oneValue ( 'SELECT Title FROM `%s` WHERE ID = %u' , [ Task :: $tableName , $taskId ]); // clone for any/each additional section while ( $studentTaskSection = array_shift ( $studentTaskSections )) { $cloneHandle = HandleBehavior :: getUniqueHandle ( Task :: class , $taskTitle ); DB :: nonQuery ( 'INSERT INTO `%1$s` (`ID`, `Handle`, `%4$s`) SELECT NULL, \"%3$s\", `%4$s` FROM `%1$s` WHERE ID = %2$u' , [ Task :: $tableName , $taskId , DB :: escape ( $cloneHandle ), implode ( '`, `' , $taskColumnNames ) ] ); $taskSectionId [ DB :: insertID ()] = $studentTaskSection [ 'SectionID' ]; printf ( \"Cloning task %u for section %u \\n \" , $taskId , $studentTaskSection [ 'SectionID' ]); } } // add SectionID column to Task table static :: addColumn ( Task :: $tableName , 'SectionID' , 'int unsigned NULL default NULL' , 'AFTER `ModifierID`' ); static :: addIndex ( Task :: $tableName , 'SectionID' ); // clone Task for each SectionID where multiple are associated printf ( \"Setting SectionID for %u Task records \\n \" , count ( $taskSectionId )); foreach ( $taskSectionId as $taskId => $sectionId ) { DB :: nonQuery ( 'UPDATE `%s` SET SectionID = %u WHERE ID = %u' , [ Task :: $tableName , $sectionId , $taskId ]); } // remove SectionID column from StudentTask table static :: dropColumn ( StudentTask :: $tableName , 'SectionID' ); // finish return static :: STATUS_EXECUTED ;","title":"Move column to parent record"},{"location":"guides/sending-email/","text":"Sending Email \u00b6 Things to cover \u00b6 Templating Using delivery services Choosing subject lines that will make each thread unique Setting from address","title":"Sending Email"},{"location":"guides/sending-email/#sending-email","text":"","title":"Sending Email"},{"location":"guides/sending-email/#things-to-cover","text":"Templating Using delivery services Choosing subject lines that will make each thread unique Setting from address","title":"Things to cover"},{"location":"guides/skeletons-from-scratch/","text":"Bootstrap a Site from Git \u00b6 Create a new site with no parent Clone the repository you want to bootstrap from to disk on the same machine hosting the new site: git clone https://github.com/JarvusInnovations/emergence-skeleton.git /tmp/emergence-skeleton-v1 Change your working directory to the newly-cloned repository: cd /tmp/emergence-skeleton-v1/ Import the contents of the repository into your newly created site, excluding root files and the .git tree: echo \"Emergence_FS::importTree('.', '/', ['exclude' => ['#^/[^/]+\\$#', '#^/\\.git(/|\\$)#'] ]);\" | sudo emergence-shell my-site-handle Be sure to substitute my-site-handle at the end Check if the imported site offers a php-config/Git.config.d file mapping the source repository into the site tree. It may be wrapped in an if condition checking the site handle that must be made to match the current site to activate it. Visit /git/status to initialize the repository link to enable pushing/pulling ongoing changes to the git repository","title":"Bootstrap a Site from Git"},{"location":"guides/skeletons-from-scratch/#bootstrap-a-site-from-git","text":"Create a new site with no parent Clone the repository you want to bootstrap from to disk on the same machine hosting the new site: git clone https://github.com/JarvusInnovations/emergence-skeleton.git /tmp/emergence-skeleton-v1 Change your working directory to the newly-cloned repository: cd /tmp/emergence-skeleton-v1/ Import the contents of the repository into your newly created site, excluding root files and the .git tree: echo \"Emergence_FS::importTree('.', '/', ['exclude' => ['#^/[^/]+\\$#', '#^/\\.git(/|\\$)#'] ]);\" | sudo emergence-shell my-site-handle Be sure to substitute my-site-handle at the end Check if the imported site offers a php-config/Git.config.d file mapping the source repository into the site tree. It may be wrapped in an if condition checking the site handle that must be made to match the current site to activate it. Visit /git/status to initialize the repository link to enable pushing/pulling ongoing changes to the git repository","title":"Bootstrap a Site from Git"},{"location":"guides/translating-sites/","text":"Localization \u00b6 Install Localization Files on Server \u00b6 First ensure the locales you\u2019d like to create localization files for are installed on your server. Check which locals are currently supported locale -a Add the locales you want (for example es ): sudo locale-gen es_US sudo locale-gen es_US.UTF-8 Run this update command sudo update-locale Restart \u2018Web\u2019 in the Emergence Portal (:9083) Marking translatable strings \u00b6 To flag text in your dwoo or php files as translatable, simply wrap the text in a gettext() or shortcut function. See full details here: http://emr.ge/docs/localization/marking Generating .pot Files \u00b6 Emergence can generate a .pot file by scanning your entire code base and pulling out all translatable strings. This file will then be consumed by Poedit where they\u2019ll be transalted and exported into .po files. To generate a .pot file, go to the site-admin portal of your website and click \u2018Generate .pot file\u2019. Generating .po Files \u00b6 Open the .pot file generated in the previous step in Poedit . Once opened, create a new translation in the language of your choice. This is where you\u2019ll actually need to translate all the strings one by one. Once finished, save the .po file out to site.po (which you\u2019ll upload in the next step). Uploading .po Files \u00b6 You\u2019ll now want to upload the site.po files generated in the previous step into each emergence site you\u2019d like to support. Upload each file using the following pattern, where es stands for Spanish. /locales/es_US.utf8/site.po The /locales directory is at the same level as your php-classes directory. If it\u2019s not already present, you\u2019ll be able to find it in the _parent directory. Note: to allow users easy access to switch back to English, you\u2019ll want to add a en_US file that contains all the bare English words. This is important because in the last step, we\u2019ll generate a select box from the supported languages and if English isn\u2019t present it won\u2019t be available. Verify Files \u00b6 To verify you\u2019ve added the files to the correct location, make sure you see all the expected languages by calling the following function: Emergence\\Locale::getAvailableLocales() Turning Localization On \u00b6 To enable localization the Emergence\\Locale::loadRequestedLocale(); needs to be called on every request. This is best done in an event handler on the Site classes\u2019 before script execute function. When this function it called, it will take the site.po file provided and generate a .mo file on the fly. This file will be saved in /emergence/sites/HANDLE/site-data/locales/ directory. /event-handlers/Site/beforeScriptExecute/localization.php <?php Emergence\\Locale::loadRequestedLocale(); Switching Locale \u00b6 Specify via query string parameter with each request \u00b6 By appending ?locale=es_US to a request URI you can select the locale es_US.utf8 for a single response. Set via client-side cookie \u00b6 By setting a cookie called locale to es_US you can switch the locale for all responses during the life of the cookie. This is the recommended technique for implementing a user-selectable locale. You can set the cookie either client-side with JavaScript or server side with a response header. Send Accept-Language header with requests \u00b6 If the browser or client application provides an Accept-Language header, Emergence will attempt to pick the closest available locale. Set default language site-wide \u00b6 /php-config/Emergence/Locale.config.d/default.php Emergence\\Locale::$default = 'en_US.utf8';","title":"Localization"},{"location":"guides/translating-sites/#localization","text":"","title":"Localization"},{"location":"guides/translating-sites/#install-localization-files-on-server","text":"First ensure the locales you\u2019d like to create localization files for are installed on your server. Check which locals are currently supported locale -a Add the locales you want (for example es ): sudo locale-gen es_US sudo locale-gen es_US.UTF-8 Run this update command sudo update-locale Restart \u2018Web\u2019 in the Emergence Portal (:9083)","title":"Install Localization Files on Server"},{"location":"guides/translating-sites/#marking-translatable-strings","text":"To flag text in your dwoo or php files as translatable, simply wrap the text in a gettext() or shortcut function. See full details here: http://emr.ge/docs/localization/marking","title":"Marking translatable strings"},{"location":"guides/translating-sites/#generating-pot-files","text":"Emergence can generate a .pot file by scanning your entire code base and pulling out all translatable strings. This file will then be consumed by Poedit where they\u2019ll be transalted and exported into .po files. To generate a .pot file, go to the site-admin portal of your website and click \u2018Generate .pot file\u2019.","title":"Generating .pot Files"},{"location":"guides/translating-sites/#generating-po-files","text":"Open the .pot file generated in the previous step in Poedit . Once opened, create a new translation in the language of your choice. This is where you\u2019ll actually need to translate all the strings one by one. Once finished, save the .po file out to site.po (which you\u2019ll upload in the next step).","title":"Generating .po Files"},{"location":"guides/translating-sites/#uploading-po-files","text":"You\u2019ll now want to upload the site.po files generated in the previous step into each emergence site you\u2019d like to support. Upload each file using the following pattern, where es stands for Spanish. /locales/es_US.utf8/site.po The /locales directory is at the same level as your php-classes directory. If it\u2019s not already present, you\u2019ll be able to find it in the _parent directory. Note: to allow users easy access to switch back to English, you\u2019ll want to add a en_US file that contains all the bare English words. This is important because in the last step, we\u2019ll generate a select box from the supported languages and if English isn\u2019t present it won\u2019t be available.","title":"Uploading .po Files"},{"location":"guides/translating-sites/#verify-files","text":"To verify you\u2019ve added the files to the correct location, make sure you see all the expected languages by calling the following function: Emergence\\Locale::getAvailableLocales()","title":"Verify Files"},{"location":"guides/translating-sites/#turning-localization-on","text":"To enable localization the Emergence\\Locale::loadRequestedLocale(); needs to be called on every request. This is best done in an event handler on the Site classes\u2019 before script execute function. When this function it called, it will take the site.po file provided and generate a .mo file on the fly. This file will be saved in /emergence/sites/HANDLE/site-data/locales/ directory. /event-handlers/Site/beforeScriptExecute/localization.php <?php Emergence\\Locale::loadRequestedLocale();","title":"Turning Localization On"},{"location":"guides/translating-sites/#switching-locale","text":"","title":"Switching Locale"},{"location":"guides/translating-sites/#specify-via-query-string-parameter-with-each-request","text":"By appending ?locale=es_US to a request URI you can select the locale es_US.utf8 for a single response.","title":"Specify via query string parameter with each request"},{"location":"guides/translating-sites/#set-via-client-side-cookie","text":"By setting a cookie called locale to es_US you can switch the locale for all responses during the life of the cookie. This is the recommended technique for implementing a user-selectable locale. You can set the cookie either client-side with JavaScript or server side with a response header.","title":"Set via client-side cookie"},{"location":"guides/translating-sites/#send-accept-language-header-with-requests","text":"If the browser or client application provides an Accept-Language header, Emergence will attempt to pick the closest available locale.","title":"Send Accept-Language header with requests"},{"location":"guides/translating-sites/#set-default-language-site-wide","text":"/php-config/Emergence/Locale.config.d/default.php Emergence\\Locale::$default = 'en_US.utf8';","title":"Set default language site-wide"}]}